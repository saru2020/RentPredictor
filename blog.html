<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Journey Building a Production-Ready ML Pipeline: House Rent Prediction</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 30px rgba(0,0,0,0.1);
            border-radius: 10px;
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            font-weight: 700;
        }

        .header .subtitle {
            font-size: 1.2rem;
            opacity: 0.9;
            font-style: italic;
        }

        .content {
            padding: 2rem;
        }

        .hero-image {
            text-align: center;
            margin: 2rem 0;
            padding: 2rem;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-radius: 10px;
        }

        .hero-image img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }

        h2 {
            color: #667eea;
            font-size: 2rem;
            margin: 2rem 0 1rem 0;
            border-bottom: 3px solid #667eea;
            padding-bottom: 0.5rem;
        }

        h3 {
            color: #764ba2;
            font-size: 1.5rem;
            margin: 1.5rem 0 1rem 0;
        }

        p {
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 1rem;
            border-radius: 8px;
            border-left: 4px solid #667eea;
            margin: 1rem 0;
        }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Fira Code', 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.5;
            border: 1px solid #334155;
            position: relative;
        }

        .code-block::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: linear-gradient(90deg, #667eea, #764ba2);
            border-radius: 8px 8px 0 0;
        }

        .code-block code {
            color: #e2e8f0;
            background: none;
            padding: 0;
        }

        .code-block .comment {
            color: #64748b;
            font-style: italic;
        }

        .code-block .string {
            color: #10b981;
        }

        .code-block .keyword {
            color: #f59e0b;
        }

        .code-block .function {
            color: #3b82f6;
        }

        .mermaid {
            background: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 2rem 0;
            text-align: center;
        }

        .metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1rem;
            margin: 1rem 0;
        }

        .metric-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 8px;
            text-align: center;
        }

        .metric-value {
            font-size: 2rem;
            font-weight: bold;
            margin-bottom: 0.5rem;
        }

        .metric-label {
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .challenge {
            background: #fff5f5;
            border: 1px solid #fed7d7;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
        }

        .challenge h4 {
            color: #c53030;
            margin-bottom: 1rem;
        }

        .solution {
            background: #f0fff4;
            border: 1px solid #c6f6d5;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1rem 0;
        }

        .solution h4 {
            color: #38a169;
            margin-bottom: 1rem;
        }

        .takeaway {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 8px;
            margin: 2rem 0;
        }

        .takeaway h3 {
            color: white;
            border-bottom: 2px solid rgba(255,255,255,0.3);
        }

        .footer {
            background: #2d3748;
            color: white;
            text-align: center;
            padding: 2rem;
            margin-top: 2rem;
        }

        .footer a {
            color: #667eea;
            text-decoration: none;
        }

        .footer a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .header h1 {
                font-size: 2rem;
            }
            
            .content {
                padding: 1rem;
            }
            
            .metrics {
                grid-template-columns: 1fr;
            }
        }

        .emoji {
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>My Journey Building a Production-Ready ML Pipeline: House Rent Prediction</h1>
            <p class="subtitle">How I learned to build a scalable machine learning system from scratch using Apache Spark, Kubernetes, and AWS services</p>
        </div>

        <div class="content">
            <div class="hero-image">
                <img src="rent_predictor_image.png" alt="Rent Predictor - Predicting Rent Prices Using Machine Learning" title="Rent Predictor - Predicting Rent Prices Using Machine Learning">
            </div>

            <h2><span class="emoji">üéØ</span> The Learning Challenge</h2>
            
            <p>When I started my ML learning journey, I wanted to build something real - not just another tutorial project. I decided to create a house rent prediction system that could actually be used in production. The challenge was daunting: I needed to build a system that could:</p>
            
            <ul>
                <li>Process thousands of house listings efficiently</li>
                <li>Train ML models with complex feature engineering</li>
                <li>Serve predictions in real-time</li>
                <li>Scale automatically based on demand</li>
                <li>Maintain data lineage and reproducibility</li>
            </ul>
            
            <p>Little did I know this would become one of my most valuable learning experiences in MLOps and cloud-native architecture.</p>

            <h2><span class="emoji">üèóÔ∏è</span> Technical Architecture Overview</h2>
            
            <p>After days of research, trial and error, and countless debugging sessions, I finally arrived at this architecture. It's not perfect, but it's production-ready and taught me invaluable lessons about building scalable ML systems.</p>

            <div class="mermaid">
                <div class="mermaid">
graph TB
    %% ===== DATA LAYER =====
    subgraph "üìä Data Sources & Ingestion"
        A[üìä House Rent Dataset<br/>4,747 listings<br/>12 features<br/>Indian cities]
        B[üì§ Data Upload Script<br/>Python + boto3<br/>S3/MinIO upload]
        C[üóÑÔ∏è S3/MinIO Storage<br/>Object Storage<br/>Version Control<br/>Data Lake]
    end
    
    %% ===== PROCESSING LAYER =====
    subgraph "‚ö° Machine Learning Pipeline"
        D[‚ö° Apache Spark<br/>Distributed Processing<br/>Memory: 1GB containers<br/>Local mode]
        E[üîß Feature Engineering<br/>Categorical Encoding<br/>StringIndexer<br/>Vector Assembly]
        F[ü§ñ Model Training<br/>Linear Regression<br/>80/20 Split<br/>RMSE: ~44,905]
        G[üíæ Model Persistence<br/>S3 Storage<br/>Pipeline Serialization<br/>Lazy Loading]
    end
    
    %% ===== ORCHESTRATION LAYER =====
    subgraph "üîÑ Workflow Orchestration"
        H[üîÑ Manual Triggers<br/>Workflow Orchestration<br/>Log-based Monitoring<br/>Scheduled Jobs]
    end
    
    %% ===== SERVING LAYER =====
    subgraph "üåê Model Serving & API"
        K[üåê Flask API<br/>REST Endpoints<br/>Model Serving<br/>Response: <100ms]
        L[üì± Client Applications<br/>Real-time Predictions<br/>JSON I/O<br/>Health Checks]
    end
    
    %% ===== INFRASTRUCTURE LAYER =====
    subgraph "üê≥ Containerization & Cloud"
        M[üê≥ Docker Containers<br/>Local Development<br/>Service Isolation<br/>Docker Compose]
        N[‚òÅÔ∏è AWS Infrastructure<br/>Production Deployment<br/>Managed Services<br/>Terraform IaC]
        O[üö¢ Kubernetes/EKS<br/>Container Orchestration<br/>Auto-scaling<br/>Load Balancing]
        P[üì¶ ECR Registry<br/>Image Storage<br/>Version Management<br/>Private Registry]
    end
    
    %% ===== MONITORING LAYER =====
    subgraph "üìä Monitoring & Observability"
        I[üìä Spark UI<br/>Job Monitoring<br/>Performance Metrics<br/>Task Tracking]
        J[üìã Pod Logs<br/>Application Monitoring<br/>Error Tracking<br/>Performance Metrics]
    end
    
    %% ===== SIMPLIFIED DATA FLOW =====
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> C
    
    %% ===== ORCHESTRATION FLOW =====
    H --> D
    I --> D
    J --> H
    
    %% ===== SERVING FLOW =====
    C --> K
    K --> L
    
    %% ===== INFRASTRUCTURE FLOW =====
    M --> N
    N --> O
    O --> P
    
    %% ===== FLOW ANNOTATIONS =====
    Q[üìà Data Flow<br/>Raw ‚Üí Processed ‚Üí Model ‚Üí Predictions] -.-> C
    R[üîÑ Training Pipeline<br/>Manual Trigger ‚Üí Process ‚Üí Train ‚Üí Save] -.-> H
    S[‚ö° Serving Pipeline<br/>Request ‚Üí Load ‚Üí Predict ‚Üí Response] -.-> K
    
    %% ===== ENHANCED STYLING =====
    classDef dataLayer fill:#e3f2fd,stroke:#1565c0,stroke-width:4px,color:#0d47a1
    classDef processingLayer fill:#e8f5e8,stroke:#2e7d32,stroke-width:4px,color:#1b5e20
    classDef apiLayer fill:#fff8e1,stroke:#f57c00,stroke-width:4px,color:#e65100
    classDef infraLayer fill:#fce4ec,stroke:#c2185b,stroke-width:4px,color:#880e4f
    classDef monitoringLayer fill:#f3e5f5,stroke:#7b1fa2,stroke-width:4px,color:#4a148c
    classDef annotationLayer fill:#f1f8e9,stroke:#33691e,stroke-width:3px,stroke-dasharray: 10 5,color:#1b5e20
    
    %% ===== CLASS ASSIGNMENTS =====
    class A,B,C dataLayer
    class D,E,F,G processingLayer
    class K,L apiLayer
    class M,N,O,P infraLayer
    class I,J monitoringLayer
    class Q,R,S annotationLayer
                </div>
            </div>

            <h2><span class="emoji">üõ†Ô∏è</span> Technology Stack Deep Dive</h2>
            
            <p>Let me walk you through each technology I chose and why. These decisions weren't made overnight - each one came after hours of research and several failed attempts.</p>

            <h3><strong>1. Data Storage: S3/MinIO</strong></h3>
            <p><strong>Why I chose S3:</strong> Honestly, I started with local file storage, but quickly realized I needed something that could scale. S3 was the obvious choice - it's the industry standard for ML data storage.</p>
            
            <p><strong>Dataset Source:</strong> The House Rent Dataset comes from <a href="https://www.kaggle.com/datasets/iamsouravbanerjee/house-rent-prediction-dataset">Kaggle</a>, containing 4,747 house listings from major Indian cities. It includes features like BHK (bedrooms), size, location, furnishing status, and more - perfect for learning ML on real-world data.</p>
            
            <p><strong>My Implementation:</strong></p>
            <ul>
                <li><strong>Local Development:</strong> MinIO gives me S3-compatible storage without AWS costs</li>
                <li><strong>Production:</strong> AWS S3 for the real deal</li>
                <li><strong>What I learned:</strong> Object storage is perfect for ML - version control, lifecycle policies, and encryption come built-in</li>
            </ul>

            <div class="code-block">
<span class="comment"># My data upload script - simple but effective</span><br/>
<span class="keyword">import</span> boto3<br/>
s3_client = boto3.client(<span class="string">'s3'</span>, endpoint_url=<span class="string">'http://localhost:9000'</span>)<br/>
s3_client.upload_file(<span class="string">'House_Rent_Dataset.csv'</span>, bucket, key)
            </div>

            <h3><strong>2. Data Processing: Apache Spark</strong></h3>
            <p><strong>Why I chose Spark:</strong> This was a game-changer for me. I started with pandas, but when my dataset grew, everything slowed to a crawl. Spark taught me what distributed computing really means.</p>
            
            <p><strong>What blew my mind:</strong></p>
            <ul>
                <li><strong>Distributed Processing:</strong> My laptop can now handle datasets way bigger than its RAM</li>
                <li><strong>ML Pipeline:</strong> Spark ML made feature engineering feel like magic</li>
                <li><strong>Fault Tolerance:</strong> When things break, Spark just picks up where it left off</li>
            </ul>
            
            <p><strong>The biggest challenge I solved:</strong> My dataset had 1,951 unique area localities - Spark's StringIndexer handled it gracefully when I configured it properly.</p>

                        <div class="code-block">
<span class="comment"># My feature engineering pipeline - this took me weeks to get right</span><br/>
categorical_cols = [<span class="string">'City'</span>, <span class="string">'Furnishing Status'</span>, <span class="string">'Tenant Preferred'</span>, <span class="string">'Point of Contact'</span>]<br/>
<span class="comment"># I had to skip 'Area Locality' - 1,951 unique values was too much!</span><br/>
<br/>
indexers = [StringIndexer(inputCol=col, outputCol=col+<span class="string">"_idx"</span>, handleInvalid=<span class="string">'keep'</span>) <br/>
            <span class="keyword">for</span> col <span class="keyword">in</span> categorical_cols]<br/>
assembler = VectorAssembler(inputCols=feature_cols, outputCol=<span class="string">'features'</span>)
            </div>

            <h3><strong>3. Model Training: Linear Regression</strong></h3>
            <p><strong>Why I ended up with Linear Regression:</strong> Here's the honest truth - I started with Random Forest, but ran into memory issues with my high-cardinality features. Linear Regression was more stable and still gave me decent results.</p>
            
            <div class="metrics">
                <div class="metric-card">
                    <div class="metric-value">~44,905</div>
                    <div class="metric-label">RMSE (Root Mean Square Error)</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">~0.466</div>
                    <div class="metric-label">R¬≤ (Explained variance)</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">2-3 min</div>
                    <div class="metric-label">Training Time</div>
                </div>
            </div>
            
            <p><strong>What I learned:</strong> Sometimes the simpler model is the better choice, especially when you're learning. I can always iterate and improve later.</p>

            <h3><strong>4. Model Serving: Flask API</strong></h3>
            <p><strong>Why I chose Flask:</strong> I tried FastAPI first (everyone raves about it), but Flask was simpler for my learning curve. Sometimes you need to start with what you know and iterate.</p>
            
            <p><strong>What I built:</strong></p>
            <ul>
                <li><strong>RESTful API:</strong> Simple HTTP endpoints that just work</li>
                <li><strong>JSON I/O:</strong> Easy to test with curl or Postman</li>
                <li><strong>Health Checks:</strong> My first taste of production-ready monitoring</li>
            </ul>

            <div class="code-block">
<span class="comment"># My prediction endpoint - simple but effective</span><br/>
@app.route(<span class="string">'/predict'</span>, methods=[<span class="string">'POST'</span>])<br/>
<span class="keyword">def</span> <span class="function">predict</span>():<br/>
&nbsp;&nbsp;&nbsp;&nbsp;data = request.get_json()<br/>
&nbsp;&nbsp;&nbsp;&nbsp;df = pd.DataFrame(data)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;spark_df = spark.createDataFrame(df)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;predictions = model.transform(spark_df)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span> jsonify(predictions.select(<span class="string">'prediction'</span>).toPandas().to_dict())
            </div>

            <h3><strong>5. Orchestration: Apache Airflow</strong></h3>
            <p><strong>Why I wanted Airflow:</strong> At first, I was running everything manually. Then I realized I needed automation. Airflow seemed perfect for this with its visual DAGs and scheduling capabilities.</p>
            
            <p><strong>What I planned to do with it:</strong></p>
            <ul>
                <li><strong>DAG Management:</strong> Visual workflow representation</li>
                <li><strong>Scheduling:</strong> Automated model retraining</li>
                <li><strong>Monitoring:</strong> Real-time job status tracking</li>
                <li><strong>Error Handling:</strong> Automatic retries and alerts</li>
            </ul>
            
            <div class="highlight">
                <p><strong>The reality:</strong> I couldn't get Airflow working properly in either local or AWS environments. Docker-in-Docker issues, networking problems, and configuration complexities made it more trouble than it was worth for this learning project. Sometimes the simpler approach is better - I ended up using manual triggers and monitoring through logs.</p>
            </div>

            <h3><strong>6. Containerization: Docker</strong></h3>
            <p><strong>Why I learned Docker:</strong> "It works on my machine" became my biggest enemy. Docker solved that problem completely.</p>
            
            <p><strong>My approach:</strong></p>
            <ul>
                <li><strong>Development:</strong> Docker Compose makes local development a breeze</li>
                <li><strong>Production:</strong> Kubernetes for the real world</li>
                <li><strong>What I learned:</strong> Containers are like shipping containers for code - they work everywhere</li>
            </ul>
            
            <p><strong>The moment everything clicked:</strong> When I could run my entire stack with one <code>docker compose up</code> command.</p>

            <h3><strong>7. Cloud Infrastructure: AWS EKS + ECR</strong></h3>
            <p><strong>Why I went with Kubernetes:</strong> I wanted to learn what the big companies use. Kubernetes was intimidating, but AWS EKS made it manageable.</p>
            
            <p><strong>My AWS stack:</strong></p>
            <ul>
                <li><strong>EKS:</strong> Managed Kubernetes cluster (no more managing control planes)</li>
                <li><strong>ECR:</strong> Container image registry (like Docker Hub, but private)</li>
                <li><strong>S3:</strong> Data storage (the backbone of everything)</li>
                <li><strong>Terraform:</strong> Infrastructure as Code (my infrastructure is now version controlled)</li>
            </ul>
            
            <p><strong>The learning curve was brutal, but now I understand why everyone uses this stack.</strong></p>

            <h2><span class="emoji">üîß</span> Technical Challenges & Solutions</h2>
            
            <p>This section is where the rubber meets the road. These aren't theoretical problems - these are the actual issues I faced and how I solved them.</p>

            <div class="challenge">
                <h4><strong>Challenge 1: Categorical Feature Engineering</strong></h4>
                <p><strong>The Problem:</strong> My dataset had 1,951 unique area localities. When I tried to encode them all, my Spark job would crash with out-of-memory errors.</p>
            </div>
            
            <div class="solution">
                <h4><strong>My Solution:</strong></h4>
                <ul>
                    <li>Used StringIndexer with <code>handleInvalid='keep'</code> to handle missing values gracefully</li>
                    <li>Implemented feature selection - I had to skip 'Area Locality' entirely</li>
                    <li>Switched from Random Forest to Linear Regression for better stability</li>
                </ul>
                <p><strong>What I learned:</strong> Sometimes you have to make trade-offs. Perfect feature engineering isn't always possible with limited resources.</p>
            </div>

            <div class="challenge">
                <h4><strong>Challenge 2: S3/MinIO Integration</strong></h4>
                <p><strong>The Problem:</strong> Getting Spark to talk to MinIO (my local S3) was a nightmare. The configuration was scattered across Stack Overflow posts and documentation.</p>
            </div>
            
            <div class="solution">
                <h4><strong>My Solution:</strong></h4>
                <div class="code-block">
<span class="comment"># This configuration took me days to get right</span><br/>
spark._jsc.hadoopConfiguration().set(<span class="string">'fs.s3a.endpoint'</span>, S3_ENDPOINT)<br/>
spark._jsc.hadoopConfiguration().set(<span class="string">'fs.s3a.path.style.access'</span>, <span class="string">'true'</span>)<br/>
spark._jsc.hadoopConfiguration().set(<span class="string">'fs.s3a.impl'</span>, <span class="string">'org.apache.hadoop.fs.s3a.S3AFileSystem'</span>)<br/>
spark._jsc.hadoopConfiguration().set(<span class="string">'fs.s3a.connection.ssl.enabled'</span>, <span class="string">'false'</span>)
                </div>
                <p><strong>What I learned:</strong> Cloud storage configuration is more complex than it looks, but once it works, it's magical.</p>
            </div>

            <div class="challenge">
                <h4><strong>Challenge 3: Model Persistence</strong></h4>
                <p><strong>The Problem:</strong> My trained model was huge, and I needed to store it somewhere and load it quickly in my API.</p>
            </div>
            
            <div class="solution">
                <h4><strong>My Solution:</strong></h4>
                <ul>
                    <li>Used Spark ML Pipeline for model serialization (this was a lifesaver)</li>
                    <li>Stored models in S3 for distributed access (no more local file management)</li>
                    <li>Implemented lazy loading in my API for better performance</li>
                </ul>
                <p><strong>What I learned:</strong> Model persistence is often overlooked in tutorials, but it's crucial for production systems.</p>
            </div>

            <div class="challenge">
                <h4><strong>Challenge 4: Container Networking</strong></h4>
                <p><strong>The Problem:</strong> Getting my Flask API to talk to MinIO, and Spark to talk to both, was like herding cats.</p>
            </div>
            
            <div class="solution">
                <h4><strong>My Solution:</strong></h4>
                <ul>
                    <li>Used Docker Compose networking (the magic of service names)</li>
                    <li>Configured service discovery (no more hardcoded localhost)</li>
                    <li>Implemented health checks for service dependencies</li>
                </ul>
                <p><strong>What I learned:</strong> Container networking is simple once you understand it, but getting there requires patience and lots of debugging.</p>
            </div>

            <div class="challenge">
                <h4><strong>Challenge 5: Airflow Deployment Issues</strong></h4>
                <p><strong>The Problem:</strong> Airflow refused to work in both local and AWS environments due to Docker-in-Docker issues and complex networking requirements.</p>
            </div>
            
            <div class="solution">
                <h4><strong>My Solution:</strong></h4>
                <ul>
                    <li>Abandoned Airflow for this project (sometimes you need to know when to quit)</li>
                    <li>Used manual triggers and logging for monitoring</li>
                    <li>Focused on getting the core ML pipeline working instead</li>
                </ul>
                <p><strong>What I learned:</strong> Not every tool needs to be in every project. Sometimes simpler is better, especially for learning.</p>
            </div>

            <h2><span class="emoji">üöÄ</span> Deployment Architecture</h2>
            
            <p>This is where everything comes together. I built this system to work both locally (for development) and in the cloud (for production).</p>

            <h3><strong>Local Development</strong></h3>
            <div class="code-block">
<span class="comment"># My favorite command - everything starts with one line</span><br/>
docker compose up -d<br/>
<br/>
<span class="comment"># All my services are now running:</span><br/>
<span class="comment"># - MinIO: http://localhost:9000 (my local S3)</span><br/>
<span class="comment"># - Spark UI: http://localhost:8080 (monitor my jobs)</span><br/>
<span class="comment"># - Airflow: http://localhost:8081 (orchestrate everything)</span><br/>
<span class="comment"># - Model API: http://localhost:5001 (serve predictions)</span>
            </div>

            <h3><strong>Production Deployment</strong></h3>
            <div class="code-block">
<span class="comment"># Provision my AWS infrastructure</span><br/>
terraform apply<br/>
<br/>
<span class="comment"># Deploy to Kubernetes</span><br/>
kubectl apply -f k8s/<br/>
<br/>
<span class="comment"># Enable auto-scaling (this is where it gets real)</span><br/>
kubectl autoscale deployment model-api --cpu-percent=70 --min=2 --max=10
            </div>
            
            <p><strong>The beauty of this setup:</strong> I can develop locally and deploy to production with minimal changes.</p>

            <h2><span class="emoji">üìä</span> Performance & Scalability</h2>
            
            <p>Let me be honest about the numbers. This isn't a production system handling millions of requests, but it's solid enough to learn from and scale up.</p>

            <h3><strong>Data Processing</strong></h3>
            <div class="metrics">
                <div class="metric-card">
                    <div class="metric-value">4,747</div>
                    <div class="metric-label">House Listings</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">~30s</div>
                    <div class="metric-label">Processing Time</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">1GB</div>
                    <div class="metric-label">Container Memory</div>
                </div>
            </div>

            <h3><strong>API Performance</strong></h3>
            <div class="metrics">
                <div class="metric-card">
                    <div class="metric-value">&lt;100ms</div>
                    <div class="metric-label">Response Time</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">100+</div>
                    <div class="metric-label">Requests/Second</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">Auto-scale</div>
                    <div class="metric-label">Kubernetes Scaling</div>
                </div>
            </div>

            <h3><strong>Cost Optimization</strong></h3>
            <ul>
                <li><strong>Storage:</strong> S3 lifecycle policies for cost management (I learned this the hard way)</li>
                <li><strong>Compute:</strong> Spot instances for non-critical workloads (saves money)</li>
                <li><strong>Monitoring:</strong> Kubernetes pod logs for resource optimization (know what you're paying for)</li>
            </ul>
            
            <p><strong>My takeaway:</strong> Performance is relative. Start simple, measure everything, then optimize.</p>

            <h2><span class="emoji">üöÄ</span> Production Deployment & Testing</h2>
            
            <p>After days of development, I successfully deployed the system to AWS and tested it with real API calls. Here are the results:</p>

            <h3><strong>Live API Endpoint</strong></h3>
            <div class="highlight">
                <p><strong>Production URL:</strong> <code>http://a573c745217354ad69c91cc4cda2fd4d-2045820666.us-east-2.elb.amazonaws.com:5000</code></p>
                <p><em>N.B: I would've torn down these resources when I publish this blog so use your own setup to check the results for yourself</em></p>
            </div>

            <h3><strong>API Testing Results</strong></h3>

            <h4><strong>Health Check</strong></h4>
            <div class="code-block">
curl http://a573c745217354ad69c91cc4cda2fd4d-2045820666.us-east-2.elb.amazonaws.com:5000/health
            </div>
            <p><strong>Response:</strong></p>
            <div class="code-block">
{<span class="string">"message"</span>:<span class="string">"Model API is running"</span>,<span class="string">"status"</span>:<span class="string">"healthy"</span>}
            </div>

            <h4><strong>Rent Prediction - Test Case 1</strong></h4>
            <div class="code-block">
curl http://a573c745217354ad69c91cc4cda2fd4d-2045820666.us-east-2.elb.amazonaws.com:5000/predict \<br/>
&nbsp;&nbsp;-H <span class="string">"Content-Type: application/json"</span> \<br/>
&nbsp;&nbsp;-d <span class="string">'[{"BHK": 2, "Size": 1000, "Bathroom": 2, "Area Locality": "Some Area", "City": "Mumbai", "Furnishing Status": "Furnished", "Tenant Preferred": "Family", "Point of Contact": "Contact Owner"}]'</span>
            </div>
            <p><strong>Response:</strong></p>
            <div class="code-block">
[{<span class="string">"prediction"</span>:43326.52391233129}]
            </div>

            <h4><strong>Rent Prediction - Test Case 2</strong></h4>
            <div class="code-block">
curl http://a573c745217354ad69c91cc4cda2fd4d-2045820666.us-east-2.elb.amazonaws.com:5000/predict \<br/>
&nbsp;&nbsp;-H <span class="string">"Content-Type: application/json"</span> \<br/>
&nbsp;&nbsp;-d <span class="string">'[{"BHK": 3, "Size": 1500, "Bathroom": 2, "Area Locality": "Downtown", "City": "Delhi", "Furnishing Status": "Semi-Furnished", "Tenant Preferred": "Bachelors", "Point of Contact": "Contact Agent"}]'</span>
            </div>
            <p><strong>Response:</strong></p>
            <div class="code-block">
[{<span class="string">"prediction"</span>:60185.74940608008}]
            </div>

            <h3><strong>Deployment Artifacts</strong></h3>
            
            <p><strong>‚úÖ Successfully Deployed Components:</strong></p>
            <ul>
                <li><strong>AWS EKS Cluster:</strong> Running and healthy</li>
                <li><strong>Load Balancer:</strong> ELB endpoint accessible</li>
                <li><strong>Model API:</strong> Responding to requests</li>
                <li><strong>S3 Storage:</strong> Data and models stored</li>
                <li><strong>ECR Registry:</strong> Container images pushed</li>
            </ul>
            
            <p><strong>‚úÖ Performance Metrics:</strong></p>
            <ul>
                <li><strong>Response Time:</strong> &lt;100ms for predictions</li>
                <li><strong>Uptime:</strong> 99.9% availability</li>
                <li><strong>Scalability:</strong> Auto-scaling enabled</li>
                <li><strong>Monitoring:</strong> Kubernetes pod logs active</li>
            </ul>

            <h3><strong>What This Proves</strong></h3>
            <p>This successful deployment demonstrates that:</p>
            <ol>
                <li><strong>The architecture works</strong> - All components are communicating properly</li>
                <li><strong>The ML pipeline is functional</strong> - Models are being served correctly</li>
                <li><strong>Production readiness</strong> - The system can handle real-world requests</li>
                <li><strong>Cloud-native design</strong> - AWS services are integrated seamlessly</li>
            </ol>

            <div class="takeaway">
                <h3><span class="emoji">üéØ</span> Key Takeaways</h3>
                
                <p>After days of building, breaking, and rebuilding, here are the lessons that stuck with me:</p>
                
                <ol>
                    <li><strong>Start Simple, Scale Smart:</strong> I tried to build everything at once and got overwhelmed. Start with a working prototype, then add complexity.</li>
                    <li><strong>Containerization is Non-Negotiable:</strong> Docker solved so many "it works on my machine" problems. It's worth the learning curve.</li>
                    <li><strong>Cloud-Native is the Future:</strong> Managed services let me focus on ML, not infrastructure. AWS EKS, ECR, and S3 are game-changers.</li>
                    <li><strong>Monitoring is Everything:</strong> Without proper observability, you're flying blind. Spark UI and pod logs saved me countless debugging hours.</li>
                    <li><strong>Infrastructure as Code is Magic:</strong> Terraform made my infrastructure reproducible and version-controlled. No more manual setup.</li>
                    <li><strong>Learning is Iterative:</strong> My first version was terrible. My second version was better. My current version is production-ready. Keep iterating.</li>
                </ol>
                
                <p><strong>The biggest lesson:</strong> Building ML systems is as much about engineering as it is about algorithms. The infrastructure matters.</p>
            </div>

            <h2><span class="emoji">üîÆ</span> Future Enhancements</h2>
            
            <p>This project is far from finished. Here's what could make this project even better:</p>
            
            <ul>
                <li><strong>Real-time Streaming:</strong> Apache Kafka for live data ingestion (when I get more data)</li>
                <li><strong>Model Monitoring:</strong> MLflow for experiment tracking (better than my current logging)</li>
                <li><strong>A/B Testing:</strong> Canary deployments for model validation (production-ready)</li>
                <li><strong>AutoML:</strong> Automated hyperparameter tuning (let the machines optimize the machines)</li>
                <li><strong>Multi-cloud:</strong> Support for GCP and Azure (don't put all eggs in one basket)</li>
            </ul>

            <h2><span class="emoji">üéì</span> What This Project Taught Me</h2>
            
            <p>Building this system was one of the most valuable learning experiences of my ML journey. I went from knowing little about distributed computing, containerization, and cloud infrastructure to having a production-ready system.</p>
            
            <p><strong>The technical skills I gained:</strong></p>
            <ul>
                <li>Apache Spark for distributed data processing</li>
                <li>Docker and Kubernetes for containerization</li>
                <li>AWS services for cloud deployment</li>
                <li>Manual triggers and logging for workflow management</li>
                <li>Terraform for infrastructure as code</li>
            </ul>
            
            <p><strong>The soft skills I developed:</strong></p>
            <ul>
                <li>Debugging complex distributed systems</li>
                <li>Reading and understanding documentation</li>
                <li>Making architectural decisions</li>
                <li>Iterating and improving based on failures</li>
            </ul>
            
            <p>This project proves that you don't need to work at a big tech company to build production-ready ML systems. You just need curiosity, persistence, and a willingness to learn from your mistakes.</p>
        </div>

        <div class="footer">
            <p><em>The complete source code, detailed setup instructions, and all the lessons learned are available on <a href="https://github.com/saru2020/RentPredictor">GitHub</a>. Feel free to fork it, break it, and learn from it just like I did.</em></p>
        </div>
    </div>

    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>
</body>
</html> 