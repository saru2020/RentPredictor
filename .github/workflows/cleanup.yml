name: 'Cleanup Infrastructure'
on:
  workflow_dispatch:
    inputs:
      cleanup_type:
        description: 'Type of cleanup to perform'
        required: true
        default: 'airflow'
        type: choice
        options:
        - airflow
        - all
        - ebs-csi

jobs:
  cleanup:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Get EKS cluster name
        run: |
          cd infra
          if terraform output eks_cluster_name > /dev/null 2>&1; then
            EKS_CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV
            echo "Found EKS cluster: $EKS_CLUSTER_NAME"
          else
            echo "Error: EKS cluster not found in Terraform state."
            exit 1
          fi
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
      
      - name: Set up Helm
        uses: azure/setup-helm@v3
      
      - name: Cleanup Airflow
        if: github.event.inputs.cleanup_type == 'airflow' || github.event.inputs.cleanup_type == 'all'
        run: |
          echo "ðŸ§¹ Cleaning up Airflow deployment..."
          
          # Check if Airflow namespace exists
          if kubectl get namespace airflow 2>/dev/null; then
            echo "ðŸ“ Airflow namespace found, cleaning up..."
            
            # Delete all PVCs first
            echo "ðŸ—‘ï¸  Deleting Airflow PVCs..."
            kubectl delete pvc --all -n airflow --force --grace-period=0 2>/dev/null || true
            
            # Delete all pods
            echo "ðŸ—‘ï¸  Deleting Airflow pods..."
            kubectl delete pods --all -n airflow --force --grace-period=0 2>/dev/null || true
            
            # Uninstall Helm release
            echo "ðŸ—‘ï¸  Uninstalling Airflow Helm release..."
            helm uninstall airflow -n airflow 2>/dev/null || true
            
            # Delete namespace
            echo "ðŸ—‘ï¸  Deleting Airflow namespace..."
            kubectl delete namespace airflow --force --grace-period=0 2>/dev/null || true
            
            echo "âœ… Airflow cleanup completed"
          else
            echo "â„¹ï¸  Airflow namespace not found, nothing to clean up"
          fi
      
      - name: Cleanup EBS CSI Driver
        if: github.event.inputs.cleanup_type == 'ebs-csi' || github.event.inputs.cleanup_type == 'all'
        run: |
          echo "ðŸ§¹ Cleaning up EBS CSI Driver..."
          
          # Check if EBS CSI driver is installed
          if helm list -n kube-system | grep aws-ebs-csi-driver; then
            echo "ðŸ“¦ EBS CSI driver found, uninstalling..."
            helm uninstall aws-ebs-csi-driver -n kube-system || true
            
            # Wait for pods to be deleted
            echo "â³ Waiting for EBS CSI driver pods to be deleted..."
            kubectl wait --for=delete pod -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system --timeout=300s 2>/dev/null || true
            
            echo "âœ… EBS CSI driver cleanup completed"
          else
            echo "â„¹ï¸  EBS CSI driver not found, nothing to clean up"
          fi
      
      - name: Cleanup EBS Volumes
        if: github.event.inputs.cleanup_type == 'all'
        run: |
          echo "ðŸ§¹ Cleaning up orphaned EBS volumes..."
          
          # Get cluster name for volume filtering
          CLUSTER_NAME=$(kubectl config current-context | sed 's/.*cluster\///')
          
          # List EBS volumes that might be orphaned
          echo "ðŸ“‹ Checking for orphaned EBS volumes..."
          aws ec2 describe-volumes \
            --filters "Name=tag:kubernetes.io/cluster/$CLUSTER_NAME,Values=owned" \
            --query 'Volumes[?State==`available`].[VolumeId,Tags[?Key==`Name`].Value|[0]]' \
            --output table || {
            echo "âš ï¸  Could not check for orphaned volumes"
          }
          
          echo "â„¹ï¸  Note: Orphaned EBS volumes should be manually deleted from AWS Console if needed"
      
      - name: Verify Cleanup
        run: |
          echo "ðŸ” Verifying cleanup results..."
          
          # Check for remaining Airflow resources
          if [ "${{ github.event.inputs.cleanup_type }}" = "airflow" ] || [ "${{ github.event.inputs.cleanup_type }}" = "all" ]; then
            echo "ðŸ“Š Checking for remaining Airflow resources..."
            kubectl get all -n airflow 2>/dev/null || echo "âœ… No Airflow resources found"
            kubectl get pvc -n airflow 2>/dev/null || echo "âœ… No Airflow PVCs found"
          fi
          
          # Check for remaining EBS CSI resources
          if [ "${{ github.event.inputs.cleanup_type }}" = "ebs-csi" ] || [ "${{ github.event.inputs.cleanup_type }}" = "all" ]; then
            echo "ðŸ“Š Checking for remaining EBS CSI resources..."
            kubectl get pods -n kube-system | grep ebs || echo "âœ… No EBS CSI pods found"
          fi
          
          echo "âœ… Cleanup verification completed"
      
      - name: Cleanup Complete
        run: |
          echo "ðŸŽ‰ Cleanup completed successfully!"
          echo ""
          echo "ðŸ“‹ Summary:"
          echo "- Cleanup type: ${{ github.event.inputs.cleanup_type }}"
          echo "- EKS cluster: $EKS_CLUSTER_NAME"
          echo "- AWS region: $AWS_REGION"
          echo ""
          echo "ðŸ’¡ Next steps:"
          echo "- If you cleaned up 'all', you may need to reinstall EBS CSI driver before deploying Airflow again"
          echo "- If you cleaned up 'airflow', you can redeploy it using the main deployment workflow" 