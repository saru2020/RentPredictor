name: 'Cleanup Infrastructure'
on:
  workflow_dispatch:
    inputs:
      cleanup_type:
        description: 'Type of cleanup to perform'
        required: true
        default: 'all'
        type: choice
        options:
          - airflow
          - ebs-csi
          - infrastructure
          - all

jobs:
  cleanup:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: "1.5.0"
      
      - name: Get EKS cluster name
        id: get_cluster_name
        run: |
          cd infra
          
          # Try to get cluster name from Terraform state
          if terraform output eks_cluster_name > /dev/null 2>&1; then
            EKS_CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV
            echo "Found EKS cluster in Terraform state: $EKS_CLUSTER_NAME"
          else
            # Try to find cluster by name pattern
            echo "Terraform state not found, searching for EKS clusters..."
            CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text | head -1)
            
            if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
              echo "EKS_CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
              echo "Found EKS cluster: $CLUSTER_NAME"
            else
              echo "No EKS clusters found with expected naming pattern"
              echo "EKS_CLUSTER_NAME=" >> $GITHUB_ENV
            fi
          fi
      
      - name: Set up Helm
        uses: azure/setup-helm@v3
      
      - name: Cleanup Kubernetes Resources
        if: github.event.inputs.cleanup_type == 'airflow' || github.event.inputs.cleanup_type == 'all'
        run: |
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "🔧 Setting up kubectl for cluster: $EKS_CLUSTER_NAME"
            aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
            
            echo "🧹 Cleaning up Airflow deployment..."
            
            # Check if Airflow namespace exists
            if kubectl get namespace airflow 2>/dev/null; then
              echo "📁 Airflow namespace found, cleaning up..."
              
              # Delete all PVCs first
              echo "🗑️  Deleting Airflow PVCs..."
              kubectl delete pvc --all -n airflow --force --grace-period=0 2>/dev/null || true
              
              # Delete all pods
              echo "🗑️  Deleting Airflow pods..."
              kubectl delete pods --all -n airflow --force --grace-period=0 2>/dev/null || true
              
              # Uninstall Helm release
              echo "🗑️  Uninstalling Airflow Helm release..."
              helm uninstall airflow -n airflow 2>/dev/null || true
              
              # Delete namespace
              echo "🗑️  Deleting Airflow namespace..."
              kubectl delete namespace airflow --force --grace-period=0 2>/dev/null || true
              
              echo "✅ Airflow cleanup completed"
            else
              echo "ℹ️  Airflow namespace not found, nothing to clean up"
            fi
          else
            echo "⚠️  No EKS cluster found, skipping Kubernetes cleanup"
          fi
      
      - name: Cleanup EBS CSI Driver
        if: (github.event.inputs.cleanup_type == 'ebs-csi' || github.event.inputs.cleanup_type == 'all') && env.EKS_CLUSTER_NAME != ''
        run: |
          echo "🧹 Cleaning up EBS CSI Driver..."
          
          # Check if EBS CSI driver is installed
          if helm list -n kube-system | grep aws-ebs-csi-driver; then
            echo "📦 EBS CSI driver found, uninstalling..."
            helm uninstall aws-ebs-csi-driver -n kube-system || true
            
            # Wait for pods to be deleted
            echo "⏳ Waiting for EBS CSI driver pods to be deleted..."
            kubectl wait --for=delete pod -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system --timeout=300s 2>/dev/null || true
            
            echo "✅ EBS CSI driver cleanup completed"
          else
            echo "ℹ️  EBS CSI driver not found, nothing to clean up"
          fi
      
      - name: Cleanup EBS Volumes
        if: github.event.inputs.cleanup_type == 'all'
        run: |
          echo "🧹 Cleaning up orphaned EBS volumes..."
          
          # Find all EKS clusters to check for volumes
          CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
          
          if [ -n "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
            for CLUSTER in $CLUSTERS; do
              if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
                echo "📋 Checking for orphaned EBS volumes for cluster: $CLUSTER"
                
                # List EBS volumes that might be orphaned
                aws ec2 describe-volumes \
                  --filters "Name=tag:kubernetes.io/cluster/$CLUSTER,Values=owned" \
                  --query 'Volumes[?State==`available`].[VolumeId,Tags[?Key==`Name`].Value|[0]]' \
                  --output table 2>/dev/null || {
                  echo "⚠️  Could not check for orphaned volumes for cluster $CLUSTER"
                }
              fi
            done
          else
            echo "ℹ️  No EKS clusters found to check for orphaned volumes"
          fi
          
          # Also check for any available volumes that might be orphaned
          echo "📋 Checking for any available EBS volumes..."
          aws ec2 describe-volumes \
            --filters "Name=state,Values=available" \
            --query 'Volumes[?length(Tags[?Key==`kubernetes.io/cluster/*`]) > `0`].[VolumeId,Tags[?Key==`Name`].Value|[0],Tags[?Key==`kubernetes.io/cluster/*`].Key|[0]]' \
            --output table 2>/dev/null || {
            echo "⚠️  Could not check for available volumes"
          }
          
          echo "ℹ️  Note: Orphaned EBS volumes should be manually deleted from AWS Console if needed"
      
      - name: Cleanup Kubernetes Resources First
        if: github.event.inputs.cleanup_type == 'all'
        run: |
          echo "🔧 Ensuring Kubernetes resources are cleaned up before infrastructure..."
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "🔧 Setting up kubectl for cluster: $EKS_CLUSTER_NAME"
            aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
            
            # Clean up any remaining Kubernetes resources
            echo "🧹 Cleaning up any remaining Kubernetes resources..."
            
            # Delete all namespaces except kube-system and default
            NAMESPACES=$(kubectl get namespaces --no-headers -o custom-columns=":metadata.name" | grep -v "kube-system\|default" 2>/dev/null || echo "")
            if [ -n "$NAMESPACES" ]; then
              for NS in $NAMESPACES; do
                echo "🗑️  Deleting namespace: $NS"
                kubectl delete namespace $NS --force --grace-period=0 2>/dev/null || true
              done
            fi
            
            # Delete any remaining PVCs
            echo "🗑️  Cleaning up any remaining PVCs..."
            kubectl delete pvc --all --all-namespaces --force --grace-period=0 2>/dev/null || true
            
            # Delete any remaining services
            echo "🗑️  Cleaning up any remaining services..."
            kubectl delete service --all --all-namespaces --force --grace-period=0 2>/dev/null || true
            
            echo "✅ Kubernetes resources cleanup completed"
          else
            echo "⚠️  No EKS cluster found, skipping Kubernetes cleanup"
          fi
      
      - name: Cleanup Infrastructure
        if: github.event.inputs.cleanup_type == 'infrastructure' || github.event.inputs.cleanup_type == 'all'
        run: |
          echo "🏗️  Cleaning up infrastructure resources..."
          cd infra
          
          # Initialize Terraform if needed
          if [ ! -d ".terraform" ]; then
            echo "🔧 Initializing Terraform..."
            terraform init
          fi
          
          # Check if state exists and has resources
          if terraform state list > /dev/null 2>&1; then
            echo "🗑️  Destroying infrastructure with Terraform..."
            
            # First, try to destroy with -refresh=false to avoid dependency issues
            echo "🔄 Attempting Terraform destroy with refresh disabled..."
            terraform destroy -auto-approve -refresh=false -var="aws_region=$AWS_REGION" || {
              echo "⚠️  Terraform destroy failed, trying to clean up dependencies first..."
              
              # Get VPC ID from Terraform state
              VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "")
              if [ -n "$VPC_ID" ]; then
                echo "🔍 Found VPC: $VPC_ID, cleaning up dependencies..."
                
                # Clean up NAT gateways
                echo "🗑️  Cleaning up NAT gateways..."
                NAT_GATEWAYS=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" --query 'NatGateways[?State!=`deleted`].NatGatewayId' --output text 2>/dev/null || echo "")
                if [ -n "$NAT_GATEWAYS" ] && [ "$NAT_GATEWAYS" != "None" ]; then
                  for NAT_GW in $NAT_GATEWAYS; do
                    if [ "$NAT_GW" != "None" ] && [ -n "$NAT_GW" ]; then
                      echo "🗑️  Deleting NAT gateway: $NAT_GW"
                      aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW || true
                    fi
                  done
                  
                  # Wait for NAT gateways to be deleted
                  echo "⏳ Waiting for NAT gateways to be deleted..."
                  sleep 60
                fi
                
                # Clean up Elastic IPs
                echo "🗑️  Cleaning up Elastic IPs..."
                EIPS=$(aws ec2 describe-addresses --filter "Name=domain,Values=vpc" --query 'Addresses[?AssociationId==null].AllocationId' --output text 2>/dev/null || echo "")
                if [ -n "$EIPS" ] && [ "$EIPS" != "None" ]; then
                  for EIP in $EIPS; do
                    if [ "$EIP" != "None" ] && [ -n "$EIP" ]; then
                      echo "🗑️  Releasing Elastic IP: $EIP"
                      aws ec2 release-address --allocation-id $EIP || true
                    fi
                  done
                fi
                
                # Clean up route table associations
                echo "🗑️  Cleaning up route table associations..."
                ROUTE_TABLES=$(aws ec2 describe-route-tables --filter "Name=vpc-id,Values=$VPC_ID" --query 'RouteTables[].RouteTableId' --output text 2>/dev/null || echo "")
                if [ -n "$ROUTE_TABLES" ] && [ "$ROUTE_TABLES" != "None" ]; then
                  for RT in $ROUTE_TABLES; do
                    if [ "$RT" != "None" ] && [ -n "$RT" ]; then
                      # Get subnet associations
                      SUBNET_ASSOCIATIONS=$(aws ec2 describe-route-tables --route-table-ids $RT --query 'RouteTables[0].Associations[?SubnetId!=null].RouteTableAssociationId' --output text 2>/dev/null || echo "")
                      if [ -n "$SUBNET_ASSOCIATIONS" ] && [ "$SUBNET_ASSOCIATIONS" != "None" ]; then
                        for ASSOC in $SUBNET_ASSOCIATIONS; do
                          if [ "$ASSOC" != "None" ] && [ -n "$ASSOC" ]; then
                            echo "🗑️  Disassociating route table: $ASSOC"
                            aws ec2 disassociate-route-table --association-id $ASSOC || true
                          fi
                        done
                      fi
                    fi
                  done
                fi
              fi
              
              # Try Terraform destroy again
              echo "🔄 Retrying Terraform destroy..."
              terraform destroy -auto-approve -var="aws_region=$AWS_REGION" || {
                echo "❌ Terraform destroy still failed, falling back to manual cleanup..."
                
                # Manual cleanup of remaining resources
                echo "🗑️  Performing manual cleanup of remaining resources..."
                
                # Clean up EKS clusters
                CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
                
                if [ -n "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
                  for CLUSTER in $CLUSTERS; do
                    if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
                      echo "🗑️  Deleting EKS cluster: $CLUSTER"
                      aws eks delete-cluster --name $CLUSTER --region $AWS_REGION || true
                    fi
                  done
                fi
                
                # Clean up S3 buckets
                BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text 2>/dev/null || echo "")
                
                if [ -n "$BUCKETS" ] && [ "$BUCKETS" != "None" ]; then
                  for BUCKET in $BUCKETS; do
                    if [ "$BUCKET" != "None" ] && [ -n "$BUCKET" ]; then
                      echo "🗑️  Deleting S3 bucket: $BUCKET"
                      aws s3 rb s3://$BUCKET --force || true
                    fi
                  done
                fi
                
                # Clean up IAM roles
                ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text 2>/dev/null || echo "")
                
                if [ -n "$ROLES" ] && [ "$ROLES" != "None" ]; then
                  for ROLE in $ROLES; do
                    if [ "$ROLE" != "None" ] && [ -n "$ROLE" ]; then
                      echo "🗑️  Deleting IAM role: $ROLE"
                      # Detach policies first
                      POLICIES=$(aws iam list-attached-role-policies --role-name $ROLE --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || echo "")
                      if [ -n "$POLICIES" ] && [ "$POLICIES" != "None" ]; then
                        for POLICY in $POLICIES; do
                          if [ "$POLICY" != "None" ] && [ -n "$POLICY" ]; then
                            aws iam detach-role-policy --role-name $ROLE --policy-arn $POLICY || true
                          fi
                        done
                      fi
                      aws iam delete-role --role-name $ROLE || true
                    fi
                  done
                fi
              }
            }
            
            echo "✅ Infrastructure cleanup completed"
          else
            echo "ℹ️  No Terraform state found, cleaning up resources manually..."
            
            # Clean up EKS clusters
            CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
            
            if [ -n "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
              for CLUSTER in $CLUSTERS; do
                if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
                  echo "🗑️  Deleting EKS cluster: $CLUSTER"
                  aws eks delete-cluster --name $CLUSTER --region $AWS_REGION || true
                fi
              done
            else
              echo "ℹ️  No EKS clusters found to delete"
            fi
            
            # Clean up S3 buckets
            BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text 2>/dev/null || echo "")
            
            if [ -n "$BUCKETS" ] && [ "$BUCKETS" != "None" ]; then
              for BUCKET in $BUCKETS; do
                if [ "$BUCKET" != "None" ] && [ -n "$BUCKET" ]; then
                  echo "🗑️  Deleting S3 bucket: $BUCKET"
                  aws s3 rb s3://$BUCKET --force || true
                fi
                done
            else
              echo "ℹ️  No S3 buckets found to delete"
            fi
            
            # Clean up IAM roles
            ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text 2>/dev/null || echo "")
            
            if [ -n "$ROLES" ] && [ "$ROLES" != "None" ]; then
              for ROLE in $ROLES; do
                if [ "$ROLE" != "None" ] && [ -n "$ROLE" ]; then
                  echo "🗑️  Deleting IAM role: $ROLE"
                  # Detach policies first
                  POLICIES=$(aws iam list-attached-role-policies --role-name $ROLE --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || echo "")
                  if [ -n "$POLICIES" ] && [ "$POLICIES" != "None" ]; then
                    for POLICY in $POLICIES; do
                      if [ "$POLICY" != "None" ] && [ -n "$POLICY" ]; then
                        aws iam detach-role-policy --role-name $ROLE --policy-arn $POLICY || true
                      fi
                    done
                  fi
                  aws iam delete-role --role-name $ROLE || true
                fi
              done
            else
              echo "ℹ️  No IAM roles found to delete"
            fi
            
            echo "✅ Manual infrastructure cleanup completed"
          fi
      
      - name: Verify Cleanup
        run: |
          echo "🔍 Verifying cleanup results..."
          
          # Check for remaining EKS clusters
          echo "📊 Checking for remaining EKS clusters..."
          REMAINING_CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
          if [ "$REMAINING_CLUSTERS" = "None" ] || [ -z "$REMAINING_CLUSTERS" ]; then
            echo "✅ No EKS clusters found"
          else
            echo "⚠️  Remaining clusters: $REMAINING_CLUSTERS"
          fi
          
          # Check for remaining S3 buckets
          echo "📊 Checking for remaining S3 buckets..."
          REMAINING_BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text 2>/dev/null || echo "")
          if [ "$REMAINING_BUCKETS" = "None" ] || [ -z "$REMAINING_BUCKETS" ]; then
            echo "✅ No S3 buckets found"
          else
            echo "⚠️  Remaining buckets: $REMAINING_BUCKETS"
          fi
          
          # Check for remaining IAM roles
          echo "📊 Checking for remaining IAM roles..."
          REMAINING_ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text 2>/dev/null || echo "")
          if [ "$REMAINING_ROLES" = "None" ] || [ -z "$REMAINING_ROLES" ]; then
            echo "✅ No IAM roles found"
          else
            echo "⚠️  Remaining roles: $REMAINING_ROLES"
          fi
          
          echo "✅ Cleanup verification completed"
      
      - name: Cleanup Complete
        run: |
          echo "🎉 Cleanup completed successfully!"
          echo ""
          echo "📋 Summary:"
          echo "- Cleanup type: ${{ github.event.inputs.cleanup_type }}"
          echo "- AWS region: $AWS_REGION"
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "- EKS cluster: $EKS_CLUSTER_NAME"
          else
            echo "- EKS cluster: Not found"
          fi
          echo ""
          echo "💡 Next steps:"
          if [ "${{ github.event.inputs.cleanup_type }}" = "all" ]; then
            echo "- All infrastructure has been cleaned up"
            echo "- You can redeploy the entire infrastructure using the main deployment workflow"
          elif [ "${{ github.event.inputs.cleanup_type }}" = "infrastructure" ]; then
            echo "- Infrastructure has been cleaned up"
            echo "- You can redeploy infrastructure using Terraform"
          else
            echo "- Kubernetes resources have been cleaned up"
            echo "- You can redeploy applications using the deployment workflows"
          fi 