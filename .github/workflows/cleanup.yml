name: 'Cleanup Infrastructure'
on:
  workflow_dispatch:
    inputs:
      cleanup_type:
        description: 'Type of cleanup to perform'
        required: true
        default: 'all'
        type: choice
        options:
          - airflow
          - ebs-csi
          - infrastructure
          - all

jobs:
  cleanup:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: "1.5.0"
      
      - name: Get EKS cluster name
        id: get_cluster_name
        run: |
          cd infra
          
          # Try to get cluster name from Terraform state
          if terraform output eks_cluster_name > /dev/null 2>&1; then
            EKS_CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV
            echo "Found EKS cluster in Terraform state: $EKS_CLUSTER_NAME"
          else
            # Try to find cluster by name pattern
            echo "Terraform state not found, searching for EKS clusters..."
            CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text | head -1)
            
            if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
              echo "EKS_CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
              echo "Found EKS cluster: $CLUSTER_NAME"
            else
              echo "No EKS clusters found with expected naming pattern"
              echo "EKS_CLUSTER_NAME=" >> $GITHUB_ENV
            fi
          fi
      
      - name: Set up Helm
        uses: azure/setup-helm@v3
      
      - name: Cleanup Kubernetes Resources
        if: github.event.inputs.cleanup_type == 'airflow' || github.event.inputs.cleanup_type == 'all'
        run: |
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "üîß Setting up kubectl for cluster: $EKS_CLUSTER_NAME"
            aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
            
            echo "üßπ Cleaning up Airflow deployment..."
            
            # Check if Airflow namespace exists
            if kubectl get namespace airflow 2>/dev/null; then
              echo "üìÅ Airflow namespace found, cleaning up..."
              
              # Delete all PVCs first
              echo "üóëÔ∏è  Deleting Airflow PVCs..."
              kubectl delete pvc --all -n airflow --force --grace-period=0 2>/dev/null || true
              
              # Delete all pods
              echo "üóëÔ∏è  Deleting Airflow pods..."
              kubectl delete pods --all -n airflow --force --grace-period=0 2>/dev/null || true
              
              # Uninstall Helm release
              echo "üóëÔ∏è  Uninstalling Airflow Helm release..."
              helm uninstall airflow -n airflow 2>/dev/null || true
              
              # Delete namespace
              echo "üóëÔ∏è  Deleting Airflow namespace..."
              kubectl delete namespace airflow --force --grace-period=0 2>/dev/null || true
              
              echo "‚úÖ Airflow cleanup completed"
            else
              echo "‚ÑπÔ∏è  Airflow namespace not found, nothing to clean up"
            fi
          else
            echo "‚ö†Ô∏è  No EKS cluster found, skipping Kubernetes cleanup"
          fi
      
      - name: Cleanup EBS CSI Driver
        if: (github.event.inputs.cleanup_type == 'ebs-csi' || github.event.inputs.cleanup_type == 'all') && env.EKS_CLUSTER_NAME != ''
        run: |
          echo "üßπ Cleaning up EBS CSI Driver..."
          
          # Check if EBS CSI driver is installed
          if helm list -n kube-system | grep aws-ebs-csi-driver; then
            echo "üì¶ EBS CSI driver found, uninstalling..."
            helm uninstall aws-ebs-csi-driver -n kube-system || true
            
            # Wait for pods to be deleted
            echo "‚è≥ Waiting for EBS CSI driver pods to be deleted..."
            kubectl wait --for=delete pod -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system --timeout=300s 2>/dev/null || true
            
            echo "‚úÖ EBS CSI driver cleanup completed"
          else
            echo "‚ÑπÔ∏è  EBS CSI driver not found, nothing to clean up"
          fi
      
      - name: Cleanup EBS Volumes
        if: github.event.inputs.cleanup_type == 'all'
        run: |
          echo "üßπ Cleaning up orphaned EBS volumes..."
          
          # Find all EKS clusters to check for volumes
          CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text)
          
          for CLUSTER in $CLUSTERS; do
            if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
              echo "üìã Checking for orphaned EBS volumes for cluster: $CLUSTER"
              
              # List EBS volumes that might be orphaned
              aws ec2 describe-volumes \
                --filters "Name=tag:kubernetes.io/cluster/$CLUSTER,Values=owned" \
                --query 'Volumes[?State==`available`].[VolumeId,Tags[?Key==`Name`].Value|[0]]' \
                --output table || {
                echo "‚ö†Ô∏è  Could not check for orphaned volumes for cluster $CLUSTER"
              }
            fi
          done
          
          echo "‚ÑπÔ∏è  Note: Orphaned EBS volumes should be manually deleted from AWS Console if needed"
      
      - name: Cleanup Infrastructure
        if: github.event.inputs.cleanup_type == 'infrastructure' || github.event.inputs.cleanup_type == 'all'
        run: |
          echo "üèóÔ∏è  Cleaning up infrastructure resources..."
          cd infra
          
          # Initialize Terraform if needed
          if [ ! -d ".terraform" ]; then
            echo "üîß Initializing Terraform..."
            terraform init
          fi
          
          # Check if state exists and has resources
          if terraform state list > /dev/null 2>&1; then
            echo "üóëÔ∏è  Destroying infrastructure with Terraform..."
            terraform destroy -auto-approve -var="aws_region=$AWS_REGION"
            echo "‚úÖ Infrastructure destroyed successfully"
          else
            echo "‚ÑπÔ∏è  No Terraform state found, cleaning up resources manually..."
            
            # Clean up EKS clusters
            CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text)
            
            for CLUSTER in $CLUSTERS; do
              if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
                echo "üóëÔ∏è  Deleting EKS cluster: $CLUSTER"
                aws eks delete-cluster --name $CLUSTER --region $AWS_REGION || true
              fi
            done
            
            # Clean up S3 buckets
            BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text)
            
            for BUCKET in $BUCKETS; do
              if [ "$BUCKET" != "None" ] && [ -n "$BUCKET" ]; then
                echo "üóëÔ∏è  Deleting S3 bucket: $BUCKET"
                aws s3 rb s3://$BUCKET --force || true
              fi
            done
            
            # Clean up IAM roles
            ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text)
            
            for ROLE in $ROLES; do
              if [ "$ROLE" != "None" ] && [ -n "$ROLE" ]; then
                echo "üóëÔ∏è  Deleting IAM role: $ROLE"
                # Detach policies first
                POLICIES=$(aws iam list-attached-role-policies --role-name $ROLE --query 'AttachedPolicies[].PolicyArn' --output text)
                for POLICY in $POLICIES; do
                  if [ "$POLICY" != "None" ] && [ -n "$POLICY" ]; then
                    aws iam detach-role-policy --role-name $ROLE --policy-arn $POLICY || true
                  fi
                done
                aws iam delete-role --role-name $ROLE || true
              fi
            done
            
            echo "‚úÖ Manual infrastructure cleanup completed"
          fi
      
      - name: Verify Cleanup
        run: |
          echo "üîç Verifying cleanup results..."
          
          # Check for remaining EKS clusters
          echo "üìä Checking for remaining EKS clusters..."
          REMAINING_CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text)
          if [ "$REMAINING_CLUSTERS" = "None" ] || [ -z "$REMAINING_CLUSTERS" ]; then
            echo "‚úÖ No EKS clusters found"
          else
            echo "‚ö†Ô∏è  Remaining clusters: $REMAINING_CLUSTERS"
          fi
          
          # Check for remaining S3 buckets
          echo "üìä Checking for remaining S3 buckets..."
          REMAINING_BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text)
          if [ "$REMAINING_BUCKETS" = "None" ] || [ -z "$REMAINING_BUCKETS" ]; then
            echo "‚úÖ No S3 buckets found"
          else
            echo "‚ö†Ô∏è  Remaining buckets: $REMAINING_BUCKETS"
          fi
          
          # Check for remaining IAM roles
          echo "üìä Checking for remaining IAM roles..."
          REMAINING_ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text)
          if [ "$REMAINING_ROLES" = "None" ] || [ -z "$REMAINING_ROLES" ]; then
            echo "‚úÖ No IAM roles found"
          else
            echo "‚ö†Ô∏è  Remaining roles: $REMAINING_ROLES"
          fi
          
          echo "‚úÖ Cleanup verification completed"
      
      - name: Cleanup Complete
        run: |
          echo "üéâ Cleanup completed successfully!"
          echo ""
          echo "üìã Summary:"
          echo "- Cleanup type: ${{ github.event.inputs.cleanup_type }}"
          echo "- AWS region: $AWS_REGION"
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "- EKS cluster: $EKS_CLUSTER_NAME"
          else
            echo "- EKS cluster: Not found"
          fi
          echo ""
          echo "üí° Next steps:"
          if [ "${{ github.event.inputs.cleanup_type }}" = "all" ]; then
            echo "- All infrastructure has been cleaned up"
            echo "- You can redeploy the entire infrastructure using the main deployment workflow"
          elif [ "${{ github.event.inputs.cleanup_type }}" = "infrastructure" ]; then
            echo "- Infrastructure has been cleaned up"
            echo "- You can redeploy infrastructure using Terraform"
          else
            echo "- Kubernetes resources have been cleaned up"
            echo "- You can redeploy applications using the deployment workflows"
          fi 