name: 'Cleanup Infrastructure'
on:
  workflow_dispatch:
    inputs:
      cleanup_type:
        description: 'Type of cleanup to perform'
        required: true
        default: 'all'
        type: choice
        options:
          - airflow
          - ebs-csi
          - infrastructure
          - all

jobs:
  cleanup:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: "1.5.0"
      
      - name: Get EKS cluster name
        id: get_cluster_name
        run: |
          cd infra
          
          # Try to get cluster name from Terraform state
          if terraform output eks_cluster_name > /dev/null 2>&1; then
            EKS_CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV
            echo "Found EKS cluster in Terraform state: $EKS_CLUSTER_NAME"
          else
            # Try to find cluster by name pattern
            echo "Terraform state not found, searching for EKS clusters..."
            CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text | head -1)
            
            if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
              echo "EKS_CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
              echo "Found EKS cluster: $CLUSTER_NAME"
            else
              echo "No EKS clusters found with expected naming pattern"
              echo "EKS_CLUSTER_NAME=" >> $GITHUB_ENV
            fi
          fi
      
      - name: Set up Helm
        uses: azure/setup-helm@v3
      
      - name: Cleanup Kubernetes Resources
        if: github.event.inputs.cleanup_type == 'airflow' || github.event.inputs.cleanup_type == 'all'
        timeout-minutes: 15
        run: |
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "🔧 Setting up kubectl for cluster: $EKS_CLUSTER_NAME"
            aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
            
            echo "🧹 Cleaning up Airflow deployment..."
            
            # Check if Airflow namespace exists
            if kubectl get namespace airflow 2>/dev/null; then
              echo "📁 Airflow namespace found, cleaning up..."
              
              # Delete all PVCs first
              echo "🗑️  Deleting Airflow PVCs..."
              kubectl delete pvc --all -n airflow --force --grace-period=0 2>/dev/null || true
              
              # Delete all pods
              echo "🗑️  Deleting Airflow pods..."
              kubectl delete pods --all -n airflow --force --grace-period=0 2>/dev/null || true
              
              # Uninstall Helm release
              echo "🗑️  Uninstalling Airflow Helm release..."
              helm uninstall airflow -n airflow 2>/dev/null || true
              
              # Delete namespace
              echo "🗑️  Deleting Airflow namespace..."
              kubectl delete namespace airflow --force --grace-period=0 2>/dev/null || true
              
              echo "✅ Airflow cleanup completed"
            else
              echo "ℹ️  Airflow namespace not found, nothing to clean up"
            fi
          else
            echo "⚠️  No EKS cluster found, skipping Kubernetes cleanup"
          fi
      
      - name: Cleanup EBS CSI Driver
        if: (github.event.inputs.cleanup_type == 'ebs-csi' || github.event.inputs.cleanup_type == 'all') && env.EKS_CLUSTER_NAME != ''
        run: |
          echo "🧹 Cleaning up EBS CSI Driver..."
          
          # Check if EBS CSI driver is installed
          if helm list -n kube-system | grep aws-ebs-csi-driver; then
            echo "📦 EBS CSI driver found, uninstalling..."
            helm uninstall aws-ebs-csi-driver -n kube-system || true
            
            # Wait for pods to be deleted
            echo "⏳ Waiting for EBS CSI driver pods to be deleted..."
            kubectl wait --for=delete pod -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system --timeout=300s 2>/dev/null || true
            
            echo "✅ EBS CSI driver cleanup completed"
          else
            echo "ℹ️  EBS CSI driver not found, nothing to clean up"
          fi
      
      - name: Cleanup EBS Volumes
        if: github.event.inputs.cleanup_type == 'all'
        run: |
          echo "🧹 Cleaning up orphaned EBS volumes..."
          
          # Find all EKS clusters to check for volumes
          CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
          
          if [ -n "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
            for CLUSTER in $CLUSTERS; do
              if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
                echo "📋 Checking for orphaned EBS volumes for cluster: $CLUSTER"
                
                # List EBS volumes that might be orphaned
                aws ec2 describe-volumes \
                  --filters "Name=tag:kubernetes.io/cluster/$CLUSTER,Values=owned" \
                  --query 'Volumes[?State==`available`].[VolumeId,Tags[?Key==`Name`].Value|[0]]' \
                  --output table 2>/dev/null || {
                  echo "⚠️  Could not check for orphaned volumes for cluster $CLUSTER"
                }
              fi
            done
          else
            echo "ℹ️  No EKS clusters found to check for orphaned volumes"
          fi
          
          # Also check for any available volumes that might be orphaned
          echo "📋 Checking for any available EBS volumes..."
          aws ec2 describe-volumes \
            --filters "Name=state,Values=available" \
            --query 'Volumes[?length(Tags[?Key==`kubernetes.io/cluster/*`]) > `0`].[VolumeId,Tags[?Key==`Name`].Value|[0],Tags[?Key==`kubernetes.io/cluster/*`].Key|[0]]' \
            --output table 2>/dev/null || {
            echo "⚠️  Could not check for available volumes"
          }
          
          echo "ℹ️  Note: Orphaned EBS volumes should be manually deleted from AWS Console if needed"
      
      - name: Cleanup Kubernetes Resources First
        if: github.event.inputs.cleanup_type == 'all'
        run: |
          echo "🔧 Ensuring Kubernetes resources are cleaned up before infrastructure..."
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "🔧 Setting up kubectl for cluster: $EKS_CLUSTER_NAME"
            aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
            
            # Clean up any remaining Kubernetes resources
            echo "🧹 Cleaning up any remaining Kubernetes resources..."
            
            # Delete all namespaces except kube-system and default
            NAMESPACES=$(kubectl get namespaces --no-headers -o custom-columns=":metadata.name" | grep -v "kube-system\|default" 2>/dev/null || echo "")
            if [ -n "$NAMESPACES" ]; then
              for NS in $NAMESPACES; do
                echo "🗑️  Deleting namespace: $NS"
                kubectl delete namespace $NS --force --grace-period=0 2>/dev/null || true
              done
            fi
            
            # Delete any remaining PVCs
            echo "🗑️  Cleaning up any remaining PVCs..."
            kubectl delete pvc --all --all-namespaces --force --grace-period=0 2>/dev/null || true
            
            # Delete any remaining services
            echo "🗑️  Cleaning up any remaining services..."
            kubectl delete service --all --all-namespaces --force --grace-period=0 2>/dev/null || true
            
            echo "✅ Kubernetes resources cleanup completed"
          else
            echo "⚠️  No EKS cluster found, skipping Kubernetes cleanup"
          fi
      
      - name: Cleanup Infrastructure
        if: github.event.inputs.cleanup_type == 'infrastructure' || github.event.inputs.cleanup_type == 'all'
        timeout-minutes: 30
        run: |
          echo "🏗️  Cleaning up infrastructure resources..."
          cd infra
          
          # Check if S3 backend bucket exists before initializing Terraform
          BACKEND_BUCKET="ml-crash-course-terraform-state"
          echo "🔍 Checking if S3 backend bucket exists: $BACKEND_BUCKET"
          
          if aws s3api head-bucket --bucket $BACKEND_BUCKET 2>/dev/null; then
            echo "✅ S3 backend bucket exists, initializing Terraform..."
            # Initialize Terraform if needed
            if [ ! -d ".terraform" ]; then
              echo "🔧 Initializing Terraform..."
              terraform init
            fi
          else
            echo "⚠️  S3 backend bucket does not exist, skipping Terraform initialization"
            echo "🗑️  Proceeding with manual cleanup of AWS resources..."
          fi
          
          # Check if state exists and has resources
          if aws s3api head-bucket --bucket $BACKEND_BUCKET 2>/dev/null && terraform state list > /dev/null 2>&1; then
            echo "🗑️  Destroying infrastructure with Terraform..."
            
            # Pre-cleanup: Remove resources that commonly cause dependency issues
            echo "🔧 Pre-cleaning resources that commonly cause dependency issues..."
            
            # Get VPC ID from Terraform state
            VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "")
            if [ -n "$VPC_ID" ]; then
              echo "🔍 Found VPC: $VPC_ID, performing pre-cleanup..."
              
              # Clean up internet gateway first (common dependency issue)
              echo "🗑️  Pre-cleaning internet gateway..."
              IGW=$(aws ec2 describe-internet-gateways --filter "Name=attachment.vpc-id,Values=$VPC_ID" --query 'InternetGateways[0].InternetGatewayId' --output text 2>/dev/null || echo "")
              if [ -n "$IGW" ] && [ "$IGW" != "None" ]; then
                echo "🗑️  Detaching internet gateway: $IGW"
                aws ec2 detach-internet-gateway --internet-gateway-id $IGW --vpc-id $VPC_ID || true
                echo "🗑️  Deleting internet gateway: $IGW"
                aws ec2 delete-internet-gateway --internet-gateway-id $IGW || true
              fi
              
              # Clean up subnets (another common dependency issue)
              echo "🗑️  Pre-cleaning subnets..."
              SUBNETS=$(aws ec2 describe-subnets --filter "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[].SubnetId' --output text 2>/dev/null || echo "")
              if [ -n "$SUBNETS" ] && [ "$SUBNETS" != "None" ]; then
                for SUBNET in $SUBNETS; do
                  if [ "$SUBNET" != "None" ] && [ -n "$SUBNET" ]; then
                    echo "🗑️  Deleting subnet: $SUBNET"
                    aws ec2 delete-subnet --subnet-id $SUBNET || true
                  fi
                done
              fi
            fi
            
            # First, try to destroy with -refresh=false to avoid dependency issues
            echo "🔄 Attempting Terraform destroy with refresh disabled..."
            terraform destroy -auto-approve -refresh=false -var="aws_region=$AWS_REGION" || {
              echo "⚠️  Terraform destroy failed, trying to clean up dependencies first..."
              
                             # Get VPC ID from Terraform state
               VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "")
               if [ -n "$VPC_ID" ]; then
                 echo "🔍 Found VPC: $VPC_ID, cleaning up dependencies..."
                 
                 # Clean up EKS node groups first (they depend on subnets)
                 echo "🗑️  Cleaning up EKS node groups..."
                 NODE_GROUPS=$(aws eks list-nodegroups --cluster-name $EKS_CLUSTER_NAME --query 'nodegroups[]' --output text 2>/dev/null || echo "")
                 if [ -n "$NODE_GROUPS" ] && [ "$NODE_GROUPS" != "None" ]; then
                   for NODE_GROUP in $NODE_GROUPS; do
                     if [ "$NODE_GROUP" != "None" ] && [ -n "$NODE_GROUP" ]; then
                       echo "🗑️  Deleting EKS node group: $NODE_GROUP"
                       aws eks delete-nodegroup --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $NODE_GROUP || true
                     fi
                   done
                   
                   # Wait for node groups to be deleted
                   echo "⏳ Waiting for EKS node groups to be deleted..."
                   sleep 120
                 fi
                 
                 # Clean up EKS cluster
                 echo "🗑️  Deleting EKS cluster: $EKS_CLUSTER_NAME"
                 aws eks delete-cluster --name $EKS_CLUSTER_NAME --region $AWS_REGION || true
                 
                 # Wait for EKS cluster to be deleted
                 echo "⏳ Waiting for EKS cluster to be deleted..."
                 sleep 180
                 
                 # Clean up load balancers that might be left behind
                 echo "🗑️  Cleaning up any remaining load balancers..."
                 LBS=$(aws elbv2 describe-load-balancers --query 'LoadBalancers[?VpcId==`'$VPC_ID'`].LoadBalancerArn' --output text 2>/dev/null || echo "")
                 if [ -n "$LBS" ] && [ "$LBS" != "None" ]; then
                   echo "⚠️  Found load balancers: $LBS"
                   for LB in $LBS; do
                     if [ "$LB" != "None" ] && [ -n "$LB" ]; then
                       echo "🗑️  Deleting load balancer: $LB"
                       aws elbv2 delete-load-balancer --load-balancer-arn $LB || true
                     fi
                   done
                   echo "⏳ Waiting for load balancers to be deleted..."
                   sleep 60
                 else
                   echo "ℹ️  No load balancers found"
                 fi
                 
                 # Clean up NAT gateways
                 echo "🗑️  Cleaning up NAT gateways..."
                 NAT_GATEWAYS=$(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID" --query 'NatGateways[?State!=`deleted`].NatGatewayId' --output text 2>/dev/null || echo "")
                 if [ -n "$NAT_GATEWAYS" ] && [ "$NAT_GATEWAYS" != "None" ]; then
                   for NAT_GW in $NAT_GATEWAYS; do
                     if [ "$NAT_GW" != "None" ] && [ -n "$NAT_GW" ]; then
                       echo "🗑️  Deleting NAT gateway: $NAT_GW"
                       aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW || true
                     fi
                   done
                   
                   # Wait for NAT gateways to be deleted
                   echo "⏳ Waiting for NAT gateways to be deleted..."
                   sleep 60
                 fi
                 
                 # Clean up Elastic IPs
                 echo "🗑️  Cleaning up Elastic IPs..."
                 EIPS=$(aws ec2 describe-addresses --filter "Name=domain,Values=vpc" --query 'Addresses[?AssociationId==null].AllocationId' --output text 2>/dev/null || echo "")
                 if [ -n "$EIPS" ] && [ "$EIPS" != "None" ]; then
                   for EIP in $EIPS; do
                     if [ "$EIP" != "None" ] && [ -n "$EIP" ]; then
                       echo "🗑️  Releasing Elastic IP: $EIP"
                       aws ec2 release-address --allocation-id $EIP || true
                     fi
                   done
                 fi
                 
                 # Clean up route table associations
                 echo "🗑️  Cleaning up route table associations..."
                 ROUTE_TABLES=$(aws ec2 describe-route-tables --filter "Name=vpc-id,Values=$VPC_ID" --query 'RouteTables[].RouteTableId' --output text 2>/dev/null || echo "")
                 if [ -n "$ROUTE_TABLES" ] && [ "$ROUTE_TABLES" != "None" ]; then
                   for RT in $ROUTE_TABLES; do
                     if [ "$RT" != "None" ] && [ -n "$RT" ]; then
                       # Get subnet associations
                       SUBNET_ASSOCIATIONS=$(aws ec2 describe-route-tables --route-table-ids $RT --query 'RouteTables[0].Associations[?SubnetId!=null].RouteTableAssociationId' --output text 2>/dev/null || echo "")
                       if [ -n "$SUBNET_ASSOCIATIONS" ] && [ "$SUBNET_ASSOCIATIONS" != "None" ]; then
                         for ASSOC in $SUBNET_ASSOCIATIONS; do
                           if [ "$ASSOC" != "None" ] && [ -n "$ASSOC" ]; then
                             echo "🗑️  Disassociating route table: $ASSOC"
                             aws ec2 disassociate-route-table --association-id $ASSOC || true
                           fi
                         done
                       fi
                     fi
                   done
                 fi
                 
                 # Clean up network interfaces
                 echo "🗑️  Cleaning up network interfaces..."
                 ENIS=$(aws ec2 describe-network-interfaces --filter "Name=vpc-id,Values=$VPC_ID" --query 'NetworkInterfaces[?Status!=`deleted`].[NetworkInterfaceId,Attachment.AttachmentId]' --output text 2>/dev/null || echo "")
                 if [ -n "$ENIS" ] && [ "$ENIS" != "None" ]; then
                   echo "📋 Network interfaces to clean up:"
                   echo "$ENIS"
                   
                   # Parse the output to get ENI IDs and attachment IDs
                   while read -r ENI_ID ATTACHMENT_ID; do
                     if [ "$ENI_ID" != "None" ] && [ -n "$ENI_ID" ] && [ "$ENI_ID" != "NetworkInterfaceId" ]; then
                       echo "🔍 Processing ENI: $ENI_ID"
                       
                       # If there's an attachment, detach it first
                       if [ "$ATTACHMENT_ID" != "None" ] && [ -n "$ATTACHMENT_ID" ] && [ "$ATTACHMENT_ID" != "AttachmentId" ]; then
                         echo "🗑️  Detaching ENI: $ENI_ID (attachment: $ATTACHMENT_ID)"
                         aws ec2 detach-network-interface --attachment-id $ATTACHMENT_ID --force || echo "⚠️  Failed to detach ENI $ENI_ID"
                         sleep 10
                       fi
                       
                       # Now try to delete the ENI
                       echo "🗑️  Deleting ENI: $ENI_ID"
                       aws ec2 delete-network-interface --network-interface-id $ENI_ID || echo "⚠️  Failed to delete ENI $ENI_ID"
                     fi
                   done <<< "$ENIS"
                 else
                   echo "ℹ️  No network interfaces found"
                 fi
                 
                 # Clean up security groups (except default)
                 echo "🗑️  Cleaning up security groups..."
                 SGS=$(aws ec2 describe-security-groups --filter "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text 2>/dev/null || echo "")
                 if [ -n "$SGS" ] && [ "$SGS" != "None" ]; then
                   for SG in $SGS; do
                     if [ "$SG" != "None" ] && [ -n "$SG" ]; then
                       echo "🗑️  Deleting security group: $SG"
                       aws ec2 delete-security-group --group-id $SG || true
                     fi
                   done
                 fi
                 
                 # Clean up internet gateway
                 echo "🗑️  Detaching and deleting internet gateway..."
                 IGW=$(aws ec2 describe-internet-gateways --filter "Name=attachment.vpc-id,Values=$VPC_ID" --query 'InternetGateways[0].InternetGatewayId' --output text 2>/dev/null || echo "")
                 if [ -n "$IGW" ] && [ "$IGW" != "None" ]; then
                   echo "🗑️  Detaching internet gateway: $IGW"
                   aws ec2 detach-internet-gateway --internet-gateway-id $IGW --vpc-id $VPC_ID || true
                   echo "🗑️  Deleting internet gateway: $IGW"
                   aws ec2 delete-internet-gateway --internet-gateway-id $IGW || true
                 fi
                 
                 # Clean up subnets
                 echo "🗑️  Cleaning up subnets..."
                 SUBNETS=$(aws ec2 describe-subnets --filter "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[].SubnetId' --output text 2>/dev/null || echo "")
                 if [ -n "$SUBNETS" ] && [ "$SUBNETS" != "None" ]; then
                   for SUBNET in $SUBNETS; do
                     if [ "$SUBNET" != "None" ] && [ -n "$SUBNET" ]; then
                       echo "🗑️  Deleting subnet: $SUBNET"
                       aws ec2 delete-subnet --subnet-id $SUBNET || true
                     fi
                   done
                 fi
               fi
              
              # Try Terraform destroy again
              echo "🔄 Retrying Terraform destroy..."
              terraform destroy -auto-approve -var="aws_region=$AWS_REGION" || {
                echo "❌ Terraform destroy still failed, falling back to manual cleanup..."
                
                # Manual cleanup of remaining resources
                echo "🗑️  Performing manual cleanup of remaining resources..."
                
                # Clean up EKS clusters
                CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
                
                if [ -n "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
                  for CLUSTER in $CLUSTERS; do
                    if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
                      echo "🗑️  Deleting EKS cluster: $CLUSTER"
                      aws eks delete-cluster --name $CLUSTER --region $AWS_REGION || true
                    fi
                  done
                fi
                
                # Clean up S3 buckets
                BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text 2>/dev/null || echo "")
                
                if [ -n "$BUCKETS" ] && [ "$BUCKETS" != "None" ]; then
                  for BUCKET in $BUCKETS; do
                    if [ "$BUCKET" != "None" ] && [ -n "$BUCKET" ]; then
                      echo "🗑️  Deleting S3 bucket: $BUCKET"
                      aws s3 rb s3://$BUCKET --force || true
                    fi
                  done
                fi
                
                # Clean up IAM roles
                ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text 2>/dev/null || echo "")
                
                if [ -n "$ROLES" ] && [ "$ROLES" != "None" ]; then
                  for ROLE in $ROLES; do
                    if [ "$ROLE" != "None" ] && [ -n "$ROLE" ]; then
                      echo "🗑️  Deleting IAM role: $ROLE"
                      # Detach policies first
                      POLICIES=$(aws iam list-attached-role-policies --role-name $ROLE --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || echo "")
                      if [ -n "$POLICIES" ] && [ "$POLICIES" != "None" ]; then
                        for POLICY in $POLICIES; do
                          if [ "$POLICY" != "None" ] && [ -n "$POLICY" ]; then
                            aws iam detach-role-policy --role-name $ROLE --policy-arn $POLICY || true
                          fi
                        done
                      fi
                      aws iam delete-role --role-name $ROLE || true
                    fi
                  done
                fi
              }
            }
            
            echo "✅ Infrastructure cleanup completed"
          else
            echo "ℹ️  No Terraform state found or S3 backend bucket doesn't exist, cleaning up resources manually..."
            
            # Clean up EKS clusters
            CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
            
            if [ -n "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
              for CLUSTER in $CLUSTERS; do
                if [ "$CLUSTER" != "None" ] && [ -n "$CLUSTER" ]; then
                  echo "🗑️  Deleting EKS cluster: $CLUSTER"
                  aws eks delete-cluster --name $CLUSTER --region $AWS_REGION || true
                fi
              done
            else
              echo "ℹ️  No EKS clusters found to delete"
            fi
            
            # Clean up S3 buckets
            BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text 2>/dev/null || echo "")
            
            if [ -n "$BUCKETS" ] && [ "$BUCKETS" != "None" ]; then
              for BUCKET in $BUCKETS; do
                if [ "$BUCKET" != "None" ] && [ -n "$BUCKET" ]; then
                  echo "🗑️  Deleting S3 bucket: $BUCKET"
                  aws s3 rb s3://$BUCKET --force || true
                fi
                done
            else
              echo "ℹ️  No S3 buckets found to delete"
            fi
            
            # Clean up IAM roles
            ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text 2>/dev/null || echo "")
            
            if [ -n "$ROLES" ] && [ "$ROLES" != "None" ]; then
              for ROLE in $ROLES; do
                if [ "$ROLE" != "None" ] && [ -n "$ROLE" ]; then
                  echo "🗑️  Deleting IAM role: $ROLE"
                  # Detach policies first
                  POLICIES=$(aws iam list-attached-role-policies --role-name $ROLE --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null || echo "")
                  if [ -n "$POLICIES" ] && [ "$POLICIES" != "None" ]; then
                    for POLICY in $POLICIES; do
                      if [ "$POLICY" != "None" ] && [ -n "$POLICY" ]; then
                        aws iam detach-role-policy --role-name $ROLE --policy-arn $POLICY || true
                      fi
                    done
                  fi
                  aws iam delete-role --role-name $ROLE || true
                fi
              done
            else
              echo "ℹ️  No IAM roles found to delete"
            fi
            
            echo "✅ Manual infrastructure cleanup completed"
          fi
          
          # Clean up the S3 backend bucket if it exists
          echo "🗑️  Cleaning up S3 backend bucket..."
          if aws s3api head-bucket --bucket $BACKEND_BUCKET 2>/dev/null; then
            echo "🗑️  Deleting S3 backend bucket: $BACKEND_BUCKET"
            # Remove any objects in the bucket first
            aws s3 rm s3://$BACKEND_BUCKET --recursive || true
            # Delete the bucket
            aws s3api delete-bucket --bucket $BACKEND_BUCKET || true
            echo "✅ S3 backend bucket cleanup completed"
          else
            echo "ℹ️  S3 backend bucket does not exist, nothing to clean up"
          fi
          
          # Additional cleanup for any remaining resources that might be stuck
          echo "🔧 Performing final cleanup of any remaining resources..."
          
          # Clean up any remaining internet gateways
          echo "🗑️  Cleaning up any remaining internet gateways..."
          IGW_LIST=$(aws ec2 describe-internet-gateways --query 'InternetGateways[?Attachments[0].State==`attached`].{IGWId:InternetGatewayId,VpcId:Attachments[0].VpcId}' --output text 2>/dev/null || echo "")
          if [ -n "$IGW_LIST" ] && [ "$IGW_LIST" != "None" ]; then
            while read -r IGW_ID VPC_ID; do
              if [ "$IGW_ID" != "None" ] && [ -n "$IGW_ID" ] && [ "$IGW_ID" != "IGWId" ]; then
                echo "🗑️  Detaching and deleting internet gateway: $IGW_ID"
                aws ec2 detach-internet-gateway --internet-gateway-id $IGW_ID --vpc-id $VPC_ID || true
                aws ec2 delete-internet-gateway --internet-gateway-id $IGW_ID || true
              fi
            done <<< "$IGW_LIST"
          fi
          
          # Clean up any remaining subnets
          echo "🗑️  Cleaning up any remaining subnets..."
          SUBNET_LIST=$(aws ec2 describe-subnets --query 'Subnets[].SubnetId' --output text 2>/dev/null || echo "")
          if [ -n "$SUBNET_LIST" ] && [ "$SUBNET_LIST" != "None" ]; then
            for SUBNET_ID in $SUBNET_LIST; do
              if [ "$SUBNET_ID" != "None" ] && [ -n "$SUBNET_ID" ]; then
                echo "🗑️  Deleting subnet: $SUBNET_ID"
                aws ec2 delete-subnet --subnet-id $SUBNET_ID || true
              fi
            done
          fi
      
      - name: Verify Cleanup
        run: |
          echo "🔍 Verifying cleanup results..."
          
          # Check for remaining EKS clusters
          echo "📊 Checking for remaining EKS clusters..."
          REMAINING_CLUSTERS=$(aws eks list-clusters --query 'clusters[?contains(name, `ml-crash-course`) || contains(name, `rentpredictor`) || contains(name, `rent-predictor`)].name' --output text 2>/dev/null || echo "")
          if [ "$REMAINING_CLUSTERS" = "None" ] || [ -z "$REMAINING_CLUSTERS" ]; then
            echo "✅ No EKS clusters found"
          else
            echo "⚠️  Remaining clusters: $REMAINING_CLUSTERS"
          fi
          
          # Check for remaining S3 buckets
          echo "📊 Checking for remaining S3 buckets..."
          REMAINING_BUCKETS=$(aws s3api list-buckets --query 'Buckets[?contains(Name, `ml-crash-course`) || contains(Name, `rentpredictor`) || contains(Name, `rent-predictor`)].Name' --output text 2>/dev/null || echo "")
          if [ "$REMAINING_BUCKETS" = "None" ] || [ -z "$REMAINING_BUCKETS" ]; then
            echo "✅ No S3 buckets found"
          else
            echo "⚠️  Remaining buckets: $REMAINING_BUCKETS"
          fi
          
          # Check for remaining IAM roles
          echo "📊 Checking for remaining IAM roles..."
          REMAINING_ROLES=$(aws iam list-roles --query 'Roles[?contains(RoleName, `ml-crash-course`) || contains(RoleName, `rentpredictor`) || contains(RoleName, `rent-predictor`)].RoleName' --output text 2>/dev/null || echo "")
          if [ "$REMAINING_ROLES" = "None" ] || [ -z "$REMAINING_ROLES" ]; then
            echo "✅ No IAM roles found"
          else
            echo "⚠️  Remaining roles: $REMAINING_ROLES"
          fi
          
          # Check for S3 backend bucket
          echo "📊 Checking for S3 backend bucket..."
          if aws s3api head-bucket --bucket "ml-crash-course-terraform-state" 2>/dev/null; then
            echo "⚠️  S3 backend bucket still exists"
          else
            echo "✅ S3 backend bucket not found"
          fi
          
          echo "✅ Cleanup verification completed"
      
      - name: Cleanup Complete
        run: |
          echo "🎉 Cleanup completed successfully!"
          echo ""
          echo "📋 Summary:"
          echo "- Cleanup type: ${{ github.event.inputs.cleanup_type }}"
          echo "- AWS region: $AWS_REGION"
          if [ -n "$EKS_CLUSTER_NAME" ]; then
            echo "- EKS cluster: $EKS_CLUSTER_NAME"
          else
            echo "- EKS cluster: Not found"
          fi
          echo ""
          echo "💡 Next steps:"
          if [ "${{ github.event.inputs.cleanup_type }}" = "all" ]; then
            echo "- All infrastructure has been cleaned up"
            echo "- You can redeploy the entire infrastructure using the main deployment workflow"
          elif [ "${{ github.event.inputs.cleanup_type }}" = "infrastructure" ]; then
            echo "- Infrastructure has been cleaned up"
            echo "- You can redeploy infrastructure using Terraform"
          else
            echo "- Kubernetes resources have been cleaned up"
            echo "- You can redeploy applications using the deployment workflows"
          fi 