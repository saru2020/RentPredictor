name: 'Complete Deployment Pipeline'
on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  # Step 1: Check and Use Existing Infrastructure
  check-infra:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
    outputs:
      s3_bucket_name: ${{ steps.infra-check.outputs.s3_bucket_name }}
      eks_cluster_name: ${{ steps.infra-check.outputs.eks_cluster_name }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
      - name: Check and Create Infrastructure
        id: infra-check
        run: |
          cd infra
          
          # Create S3 bucket for Terraform state if it doesn't exist
          aws s3 mb s3://ml-crash-course-terraform-state --region ${{ secrets.AWS_REGION }} || true
          
          # Initialize Terraform with backend using dynamic region
          terraform init \
            -backend-config="region=${{ secrets.AWS_REGION }}"
          
          # Check if resources already exist
          echo "Checking for existing resources..."
          
          S3_EXISTS=false
          IAM_EXISTS=false
          EKS_EXISTS=false
          
          # Check if S3 bucket exists
          if aws s3api head-bucket --bucket ml-crash-course-data 2>/dev/null; then
            echo "✅ S3 bucket 'ml-crash-course-data' already exists"
            S3_EXISTS=true
          else
            echo "❌ S3 bucket 'ml-crash-course-data' does not exist"
          fi
          
          # Check if IAM role exists
          if aws iam get-role --role-name ml-crash-course-cluster-cluster-role 2>/dev/null; then
            echo "✅ IAM role 'ml-crash-course-cluster-cluster-role' already exists"
            IAM_EXISTS=true
          else
            echo "❌ IAM role 'ml-crash-course-cluster-cluster-role' does not exist"
          fi
          
          # Check if EKS cluster exists
          if aws eks describe-cluster --name ml-crash-course-cluster --region ${{ secrets.AWS_REGION }} 2>/dev/null; then
            echo "✅ EKS cluster 'ml-crash-course-cluster' already exists"
            EKS_EXISTS=true
          else
            echo "❌ EKS cluster 'ml-crash-course-cluster' does not exist"
          fi
          
          # Use the variables we checked to determine if Terraform is needed
          if [ "$S3_EXISTS" = true ] && [ "$IAM_EXISTS" = true ] && [ "$EKS_EXISTS" = true ]; then
            echo "🎉 All required resources already exist! Skipping Terraform execution."
            echo "Using existing resources:"
            echo "  - S3 Bucket: ml-crash-course-data"
            echo "  - IAM Role: ml-crash-course-cluster-cluster-role"
            echo "  - EKS Cluster: ml-crash-course-cluster"
          else
            echo "🚀 Some resources don't exist. Running Terraform to create missing infrastructure..."
            
            # Create/update infrastructure (will handle existing resources gracefully)
            echo "Planning infrastructure..."
            PLAN_OUTPUT=$(terraform plan \
              -var="aws_region=${{ secrets.AWS_REGION }}" \
              -var="s3_bucket_name=ml-crash-course-data" \
              -var="eks_cluster_name=ml-crash-course-cluster" \
              -out=tfplan 2>&1)
            PLAN_EXIT_CODE=$?
            
            if [ $PLAN_EXIT_CODE -ne 0 ]; then
              echo "⚠️  Plan completed with warnings:"
              echo "$PLAN_OUTPUT"
            else
              echo "✅ Plan completed successfully"
            fi
            
            echo "Applying infrastructure..."
            
            # Check if plan file exists
            if [ ! -f "tfplan" ]; then
              echo "⚠️  Plan file not found, applying directly..."
              APPLY_OUTPUT=$(terraform apply -auto-approve \
                -var="aws_region=${{ secrets.AWS_REGION }}" \
                -var="s3_bucket_name=ml-crash-course-data" \
                -var="eks_cluster_name=ml-crash-course-cluster" 2>&1)
              APPLY_EXIT_CODE=$?
            else
              echo "Using plan file for apply..."
              APPLY_OUTPUT=$(terraform apply tfplan 2>&1)
              APPLY_EXIT_CODE=$?
            fi
            
            if [ $APPLY_EXIT_CODE -ne 0 ]; then
              # Check for specific "already exists" errors
              if echo "$APPLY_OUTPUT" | grep -q "BucketAlreadyOwnedByYou\|EntityAlreadyExists\|ResourceInUseException"; then
                echo "⚠️  Apply completed with warnings (some resources already exist):"
                echo "$APPLY_OUTPUT" | grep -E "(BucketAlreadyOwnedByYou|EntityAlreadyExists|ResourceInUseException)" || true
                echo "This is expected behavior - continuing with deployment..."
              else
                echo "❌ Apply failed with unexpected errors:"
                echo "$APPLY_OUTPUT"
                echo "Full apply output for debugging:"
                echo "$APPLY_OUTPUT"
                exit 1
              fi
            else
              echo "✅ Apply completed successfully"
            fi
          fi
          
          echo "Getting infrastructure outputs..."
          
          # Use the variables we checked to determine the resource names
          if [ "$S3_EXISTS" = true ] && [ "$IAM_EXISTS" = true ] && [ "$EKS_EXISTS" = true ]; then
            # All resources exist, use the known names
            S3_BUCKET_NAME="ml-crash-course-data"
            EKS_CLUSTER_NAME="ml-crash-course-cluster"
            echo "Using known resource names (Terraform skipped):"
          else
            # Some resources were created, get from Terraform outputs
            S3_BUCKET_NAME=$(terraform output -raw s3_bucket_name 2>/dev/null || echo "ml-crash-course-data")
            EKS_CLUSTER_NAME=$(terraform output -raw eks_cluster_name 2>/dev/null || echo "ml-crash-course-cluster")
            echo "Using Terraform outputs:"
          fi
          
          echo "s3_bucket_name=$S3_BUCKET_NAME" >> $GITHUB_OUTPUT
          echo "eks_cluster_name=$EKS_CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "Using S3 Bucket: $S3_BUCKET_NAME"
          echo "Using EKS Cluster: $EKS_CLUSTER_NAME"

  # Step 2: Upload Dataset to S3
  upload-data:
    runs-on: ubuntu-latest
    needs: check-infra
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
      S3_BUCKET: ${{ needs.check-infra.outputs.s3_bucket_name }}
      S3_KEY: House_Rent_Dataset.csv
      LOCAL_PATH: data/House_Rent_Dataset.csv
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install boto3
      - name: Upload data to S3
        run: |
          echo "Uploading to S3 bucket: $S3_BUCKET"
          python upload_data_to_s3.py

  # Step 3: Build and Push Docker Images
  build-and-push:
    runs-on: ubuntu-latest
    needs: [check-infra, upload-data]
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
      ECR_REPO_SPARK: ml-crash-course-spark
      ECR_REPO_API: ml-crash-course-api
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Create ECR repositories
        run: |
          # Create Spark jobs repository
          aws ecr describe-repositories --repository-names $ECR_REPO_SPARK --region $AWS_REGION || \
          aws ecr create-repository --repository-name $ECR_REPO_SPARK --region $AWS_REGION
          
          # Create Model API repository
          aws ecr describe-repositories --repository-names $ECR_REPO_API --region $AWS_REGION || \
          aws ecr create-repository --repository-name $ECR_REPO_API --region $AWS_REGION
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      - name: Build and push Spark job image
        run: |
          docker build -t $ECR_REPO_SPARK:latest -f spark_jobs/Dockerfile ./spark_jobs
          docker tag $ECR_REPO_SPARK:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO_SPARK:latest
          docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO_SPARK:latest
      - name: Build and push Model API image
        run: |
          docker build -t $ECR_REPO_API:latest -f model_api/Dockerfile ./model_api
          docker tag $ECR_REPO_API:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO_API:latest
          docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO_API:latest

  # Step 4: Deploy to EKS
  deploy-to-eks:
    runs-on: ubuntu-latest
    needs: [check-infra, upload-data, build-and-push]
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
      ECR_REPO_API: ml-crash-course-api
      EKS_CLUSTER_NAME: ${{ needs.check-infra.outputs.eks_cluster_name }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      - name: Update kubeconfig
        run: |
          echo "Configuring kubectl for EKS cluster: $EKS_CLUSTER_NAME"
          aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
      - name: Set up Helm
        uses: azure/setup-helm@v3
      - name: Deploy Airflow via Helm
        run: |
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          helm upgrade --install airflow apache-airflow/airflow -f k8s/airflow/values.yaml --namespace airflow --create-namespace
      - name: Install yq
        run: |
          sudo wget https://github.com/mikefarah/yq/releases/download/v4.43.1/yq_linux_amd64 -O /usr/bin/yq
          sudo chmod +x /usr/bin/yq
      - name: Substitute ECR_IMAGE_URI in model API deployment
        run: |
          ECR_IMAGE_URI="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPO_API:latest"
          yq e '.spec.template.spec.containers[0].image = env(ECR_IMAGE_URI)' k8s/model_api/deployment.yaml > k8s/model_api/deployment.sub.yaml
      - name: Deploy Model API
        run: |
          kubectl apply -f k8s/model_api/deployment.sub.yaml
          kubectl apply -f k8s/model_api/service.yaml
          kubectl patch service model-api -p '{"spec": {"ports": [{"port": 5001, "targetPort": 5000}]}}'
      - name: Deployment Complete
        run: |
          echo "🎉 Complete deployment pipeline finished successfully!"
          echo "Infrastructure: ✅"
          echo "Data uploaded: ✅"
          echo "Applications deployed: ✅" 